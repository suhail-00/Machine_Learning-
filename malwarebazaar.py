import os
import json
import time
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm

BASE_URL = "https://bazaar.abuse.ch/browse/"

# Folder to store scraped data
RAW_DIR = os.path.join("data", "raw", "malwarebazaar")
os.makedirs(RAW_DIR, exist_ok=True)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; SafeScraperBot/1.0; +https://yourproject.org)"
}

def fetch_malwarebazaar_samples(pages=2, delay=5):
    """
    Scrapes malware sample metadata from MalwareBazaar browse pages.
    Only collects hashes, file types, and tags â€” no binary files.
    """
    all_data = []

    for page in tqdm(range(1, pages + 1), desc="Scraping MalwareBazaar"):
        url = f"{BASE_URL}?page={page}"
        print(f"Fetching page {page}: {url}")
        response = requests.get(url, headers=HEADERS, timeout=10)

        if response.status_code != 200:
            print(f"Failed to fetch page {page}: {response.status_code}")
            time.sleep(delay)
            continue

        soup = BeautifulSoup(response.text, "html.parser")
        table = soup.find("table", {"class": "table"})

        if not table:
            print("No table found on page, stopping.")
            break

        rows = table.find_all("tr")[1:]  # Skip header

        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 5:
                continue

            data = {
                "sha256_hash": cols[0].text.strip(),
                "file_type": cols[2].text.strip(),
                "upload_date": cols[3].text.strip(),
                "tags": cols[4].text.strip(),
                "source_page": url
            }
            all_data.append(data)

            # Save raw JSON per sample
            sha = data["sha256_hash"]
            filepath = os.path.join(RAW_DIR, f"{sha}.json")
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=4)

        time.sleep(delay)

    print(f"Scraped {len(all_data)} samples.")
    return all_data


if __name__ == "__main__":
    data = fetch_malwarebazaar_samples(pages=3)
    # Optional: Save combined dataset
    with open(os.path.join(RAW_DIR, "combined_samples.json"), "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4)
